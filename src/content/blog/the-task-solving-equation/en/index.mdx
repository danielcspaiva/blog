---
title: "The Task-Solving Equation"
description: In the AI era, knowledge, context, and tools multiply to determine the probability of success.
date: 2025-08-20
tags: [blog, ai, communication, tech]
locale: en
hideTableOfContents: false
---

import Callout from "@/components/Callout.astro";
import HumanVsAIRadarChart from "@/components/HumanVsAIRadarChart.astro";

## TL;DR

Task success depends on three factors: knowledge, context, and tools. They multiply, not add, so if any one is missing, the probability of success collapses.
- Humans: Usually limited in knowledge, but rich in context (through shared understanding) and with practically unlimited access to tools.
- AIs: Vast in knowledge, but starve without explicit context and have narrow, wired-in tool access.

That’s why in the AI era, context is the bottleneck. Clear, structured context is the leverage that turns AI from unreliable guesswork into a reliable collaborator.

---

## The equation

Imagine someone gives you a task. What determines whether you succeed? We can break it down into three main factors:

1. **<span class="term-knowledge">Knowledge (K)</span>**: what you already know. This includes domain expertise, learned skills, past experiences, and accumulated wisdom.  
2. **<span class="term-context">Context (C)</span>**: information about this specific task. The who, what, when, where, and why that turns a generic request into an actionable one.  
3. **<span class="term-tools">Tools (T)</span>**: what you can use to get it done. Instruments, software, processes, collaborators, or your ability to invent new solutions.

After thinking about how this factors interact with each other, I would like to propose the following equation:

<p class="text-xl font-semibold rounded-lg p-2">p(task) = <span class="term-knowledge-weight">α</span>·<span class="term-knowledge">K</span> × <span class="term-context-weight">β</span>·<span class="term-context">C</span> × <span class="term-tools-weight">γ</span>·<span class="term-tools">T</span> + <span class="term-baseline">δ</span></p>

Each term matters differently depending on the situation:

- **<span class="term-knowledge-weight">α</span> (<span class="term-knowledge">knowledge</span> weight):** Higher for specialized domains (e.g. surgery α≈0.9), lower for routine tasks (data entry α≈0.3).  
- **<span class="term-context-weight">β</span> (<span class="term-context">context</span> weight):** Extremely high for debugging (β≈0.95), moderate for creative tasks (β≈0.6).  
- **<span class="term-tools-weight">γ</span> (<span class="term-tools">tools</span> weight):** Strong in technical work (γ≈0.8), lower in pure reasoning (γ≈0.4).  
- **<span class="term-baseline">δ (baseline)</span>:** The small chance of success through luck or brute force. Usually tiny (δ≈0.05) but never zero.  

This is a **multiplicative** relationship: if <span class="term-knowledge">K</span>, <span class="term-context">C</span>, or <span class="term-tools">T</span> approach zero, the probability collapses toward δ. You can't swap missing <span class="term-context">context</span> with more <span class="term-knowledge">knowledge</span> or better <span class="term-tools">tools</span>, to successfully solve the task you need all three.

### Why multiplication matters

You can't compensate for missing context with more knowledge or better tools. A surgeon with perfect training and equipment still fails without knowing what procedure to perform. Similarly, AI with vast knowledge fails on vague prompts like "Deploy failed. Fix it!" without environment details, logs, or recent changes.

---

## Humans × AI

The same three factors (knowledge, context, and tools) apply to both humans and AIs, but they manifest in very different ways.

<HumanVsAIRadarChart />

Human experts still outperform AIs when you consider knowledge, context, and tools together. At the same time, AIs already surpass the average human on raw knowledge. As models improve, success will hinge less on adding knowledge and more on supplying precise context and access to the right tools.

| Factor | **Humans** | **AIs** |
|--------|------------|---------|
| **Knowledge** | Limited but deep. Built through practice, learning, and lived experience. Intuitive connections over time. | Vast but shallow. Pretrained across the internet, with broad recall but no lived experience or intuition. |
| **Context** | Inferred almost automatically: native language, tone of voice, shared history, assumptions, cultural priors. | Minimal unless explicitly provided. Models lack your personal history, environment, or shared assumptions. Every relevant detail must be stated. |
| **Tools** | Practically unlimited. We adapt, combine, or invent tools on demand. | Limited to what’s wired in (APIs, code exec, browsers, MCPs). Cannot create new tools, but can use existing ones to gather missing context when guided. |

**The result:** Humans usually struggle with *knowledge*, which is why education takes years. AIs struggle with *context*, which is why prompt (or context) engineering exists.

---

## Applying the equation

Let's see the equation in action with a debugging scenario. Debugging tasks have high context dependency, so I'll use these weights:

- **<span class="term-knowledge-weight">α</span>** = 0.7 (<span class="term-knowledge">knowledge</span> weight): Moderate, since debugging requires domain knowledge but isn't highly specialized
- **<span class="term-context-weight">β</span>** = 0.9 (<span class="term-context">context</span> weight): Very high, debugging is nearly impossible without specifics
- **<span class="term-tools-weight">γ</span>** = 0.8 (<span class="term-tools">tools</span> weight): High, debugging benefits greatly from logs, version control, etc.
- **<span class="term-baseline">δ</span>** = 0.05 (baseline): Small chance of luck or brute force success

Now let's compare two requests:

**Before (poor context):**  
> "Vercel deploy failed. Help!"

Almost no information: which deploy? what error? what environment?

With minimal <span class="term-context">context</span> (**<span class="term-context">C</span>** ≈ 0.2), decent <span class="term-knowledge">knowledge</span> (**<span class="term-knowledge">K</span>** ≈ 0.8), and good <span class="term-tools">tools</span> (**<span class="term-tools">T</span>** ≈ 0.9):  
<p class="text-xl font-semibold rounded-lg p-2">p = <span class="term-knowledge-weight">0.7</span>×<span class="term-knowledge">0.8</span> × <span class="term-context-weight">0.9</span>×<span class="term-context">0.2</span> × <span class="term-tools-weight">0.8</span>×<span class="term-tools">0.9</span> + <span class="term-baseline">0.05</span> ≈ 0.12</p>
<p class="text-xl font-semibold rounded-lg p-2">Success probability = 12%</p>

**After (good context, structured):**  
> **Goal:** Get production build passing again.  
> **Symptom:** Deploys fail since commit `b7c9d1e`, error: `Module not found: '@/components/Button'`.  
> **Env:** Next.js 14, Node 20, Vercel build image `2025.07`.  
> **Tried:** Cleared `.next` and `.vercel`, redeployed twice, confirmed local build works.  
> **Links:** Failing build log [link], commit `b7c9d1e`.  
> **Ask:** Identify cause (path alias vs case-sensitive import?) and suggest fix or rollback.  

Now with rich <span class="term-context">context</span> (**<span class="term-context">C</span>** ≈ 0.9), same <span class="term-knowledge">knowledge</span> (**<span class="term-knowledge">K</span>** ≈ 0.8), and same <span class="term-tools">tools</span> (**<span class="term-tools">T</span>** ≈ 0.9):  
<p class="text-xl font-semibold rounded-lg p-2">p = <span class="term-knowledge-weight">0.7</span>×<span class="term-knowledge">0.8</span> × <span class="term-context-weight">0.9</span>×<span class="term-context">0.9</span> × <span class="term-tools-weight">0.8</span>×<span class="term-tools">0.9</span> + <span class="term-baseline">0.05</span> ≈ 0.38</p>
<p class="text-xl font-semibold rounded-lg p-2">Success probability = 38%</p>

**Context tripled the odds of success.** The same expert, same tools, but clear context transforms a likely failure into a reasonable chance of success.

---

## If context is so critical, why do we neglect it?

We neglect context because we carry it invisibly. Our assumptions, mental models, and shared history feel so obvious that we forget others (or AIs) don’t share them.

On top of that, context feels like overhead. Typing “Deploy failed. Fix it!” feels efficient, even though it guarantees multiple back-and-forths later.

These shortcuts work (somewhat) in human-to-human interaction, because we’re good at inferring context from tone, body language, and shared experience. But with AI, the problem gets amplified.

Another reason is our fundamental misunderstanding of how models work:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">What I find endlessly fascinating:<br /><br />Some engineers really can&#39;t seem to grasp that LLMs are non-deterministic and how to build software taking that into account.<br /><br />For others it immediately clicked, but for some it seems like there&#39;s a real mental barrier to accept it.</p>&mdash; Thorsten Ball (@thorstenball) <a href="https://twitter.com/thorstenball/status/1956017792484040731?ref_src=twsrc%5Etfw">August 14, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

LLMs have quirks that make context essential:
- **Nondeterminism:** The same prompt can yield different outputs. Good context narrows the range of possible answers, reducing randomness.
- **Knowledge cutoffs:** Models don't know anything past their training date. Explicit context about versions, updates, or recent changes patches that gap.
- **Hallucinations:** When missing details, models make things up with confidence. Rich context grounds them, leaving less room to fabricate.

All three quirks share the same fix: clear, structured context turns uncertain guesses into reliable solutions.

---

## The Rise of Context Engineering

AI has fundamentally shifted what skills matter. In the pre-AI era, success required accumulating knowledge and acquiring better tools. Now, with models that already possess vast knowledge and expanding tool access, **context engineering** has emerged as the critical skill.

Tobi Lütke captured this shift perfectly:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">I really like the term "context engineering" over prompt engineering. <br /><br />It describes the core skill better: the art of providing all the context for the task to be plausibly solvable by the LLM.</p>&mdash; tobi lutke (@tobi) <a href="https://twitter.com/tobi/status/1935533422589399127?ref_src=twsrc%5Etfw">June 19, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Context engineering is becoming as fundamental as programming was in the software era. Just as developers learned to structure code, debug systems, and design architectures, we now need to learn how to:

- **Structure information** for AI consumption
- **Anticipate missing context** that humans take for granted  
- **Design context frameworks** that scale across different tasks and domains
- **Debug context gaps** when AI outputs fall short

The professionals who master context engineering will have the same advantage early programmers had: they'll be able to reliably harness the most powerful tools of their era while others struggle with inconsistent results.

---

## Conclusion

The **task-solving equation** highlights the shift of the AI era. Success no longer hinges on accumulating more knowledge or better tools. Those are increasingly commoditized. What’s scarce is the ability to provide rich, explicit context.

Humans succeed despite limited knowledge because we’re rich in context and adaptable with tools. AIs succeed only when we deliberately supply the missing context. Every assumption, every nuance, every artifact matters.

As Guillermo Rauch observed:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">Imagine success being determined purely based on the quality of your thoughts. That's the promise of AI</p>&mdash; Guillermo Rauch (@rauchg) <a href="https://twitter.com/rauchg/status/1956356467898114443?ref_src=twsrc%5Etfw">August 15, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


The quality of your thoughts (expressed through clear context) now determines success.  

**Clear context is leverage. In a world where knowledge is cheap and tools are abundant, context is the multiplier that decides outcomes.**