---
title: "The Task-Solving Equation: Why Context Is the Bottleneck"
description: Knowledge, context, and tools multiply—not add—in determining success. In the AI era, context is the scarce multiplier.
date: 2025-08-20
tags: [blog, ai, communication, tech, problem-solving]
locale: en
hideTableOfContents: false
---

import Callout from "@/components/Callout.astro";
import HumanVsAIRadarChart from "@/components/HumanVsAIRadarChart.astro";

## TL;DR

Task success depends on three factors: knowledge (K), context (C), and tools (T). They multiply—not add—so if any one is missing, the probability of success collapses.
- Humans: Usually limited in knowledge, but rich in context (through shared understanding) and with practically unlimited access to tools.
- AIs: Vast in knowledge, but starve without explicit context and have narrow, wired-in tool access.

That’s why in the AI era, context is the bottleneck. Clear, structured context is the leverage that turns AI from unreliable guesswork into a reliable collaborator.

---

## What is the probability of solving a task?

Imagine someone gives you a task. What determines whether you succeed? Three factors:

1. **<span class="term-knowledge">Knowledge (K)</span>** — what you already know. This includes domain expertise, learned skills, past experiences, and accumulated wisdom.  
2. **<span class="term-context">Context (C)</span>** — information about this specific task. The who, what, when, where, and why that turns a generic request into an actionable one.  
3. **<span class="term-tools">Tools (T)</span>** — what you can use to get it done. Instruments, software, processes, collaborators, or your ability to invent new solutions.

I would like to propose the following equation:

<p class="text-center text-xl font-semibold rounded-lg p-8">p(task) = <span class="term-knowledge-weight">α</span>·<span class="term-knowledge">K</span> × <span class="term-context-weight">β</span>·<span class="term-context">C</span> × <span class="term-tools-weight">γ</span>·<span class="term-tools">T</span> + <span class="term-baseline">δ</span></p>

Each term matters differently depending on the situation:

- **<span class="term-knowledge-weight">α</span> (<span class="term-knowledge">knowledge</span> weight):** Higher for specialized domains (e.g. surgery α≈0.9), lower for routine tasks (data entry α≈0.3).  
- **<span class="term-context-weight">β</span> (<span class="term-context">context</span> weight):** Extremely high for debugging (β≈0.95), moderate for creative tasks (β≈0.6).  
- **<span class="term-tools-weight">γ</span> (<span class="term-tools">tools</span> weight):** Strong in technical work (γ≈0.8), lower in pure reasoning (γ≈0.4).  
- **<span class="term-baseline">δ (baseline)</span>:** The small chance of success through luck or brute force. Usually tiny (δ≈0.05) but never zero.  

This is a **multiplicative** relationship: if <span class="term-knowledge">K</span>, <span class="term-context">C</span>, or <span class="term-tools">T</span> approach zero, the probability collapses toward δ. You can’t swap missing <span class="term-context">context</span> with more <span class="term-knowledge">knowledge</span> or better <span class="term-tools">tools</span>—each factor depends on the others.

---

## Why Multiplication Matters

Addition would imply you could “make up” for poor context with more knowledge or better tools. But real problem-solving doesn’t work that way.

Consider a surgeon with years of training and the best equipment. If they enter an operating room with no patient history, no diagnosis, no idea what procedure is needed—their expertise collapses to uselessness. Multiplication drives the probability toward zero.

The same applies to AI. When you say:  
> “Deploy failed. Help!”

The AI may have vast knowledge of deployments and access to debugging tools, but without context—your environment, logs, recent changes—success probability collapses.  

*Yes, tools can help collect missing context (reading logs, inspecting commits), but only if the model is directed toward them. Baseline context is still essential.*

---

## Humans × AI

The same three factors—knowledge, context, and tools—apply to both humans and AIs, but they manifest in very different ways.

<HumanVsAIRadarChart />

| Factor | **Humans** | **AIs** |
|--------|------------|---------|
| **Knowledge** | Limited but deep. Built through practice, learning, and lived experience. Intuitive connections over time. | Vast but shallow. Pretrained across the internet, with broad recall but no lived experience or intuition. |
| **Context** | Inferred almost automatically: native language, tone of voice, shared history, assumptions, cultural priors. | Minimal unless explicitly provided. Models lack your personal history, environment, or shared assumptions. Every relevant detail must be stated. |
| **Tools** | Practically unlimited. We adapt, combine, or invent tools on demand. | Limited to what’s wired in (APIs, code exec, browsers, MCPs). Cannot create new tools, but can use existing ones to gather missing context when guided. |

**The result:** Humans usually struggle with *knowledge*—that’s why education takes years. AIs struggle with *context*—that’s why prompt (or context) engineering exists.

---

## Before → After Example

Here’s how context transforms an unsolvable request into a tractable one:

**Before (poor context):**  
> “Vercel deploy failed. Help!”

Almost no information: which deploy? what error? what environment? Plugging in rough weights for debugging tasks (α≈0.7, β≈0.9, γ≈0.8, δ≈0.05):  
`p ≈ 0.11`

**After (good context, structured):**  
> **Goal:** Get production build passing again.  
> **Symptom:** Deploys fail since commit `b7c9d1e`, error: `Module not found: '@/components/Button'`.  
> **Env:** Next.js 14, Node 20, Vercel build image `2025.07`.  
> **Tried:** Cleared `.next` and `.vercel`, redeployed twice, confirmed local build works.  
> **Links:** Failing build log [link], commit `b7c9d1e`.  
> **Ask:** Identify cause (path alias vs case-sensitive import?) and suggest fix or rollback.  

Now:  
`p ≈ 0.52`

The difference isn’t incremental—it’s transformative. Context multiplies the odds.

---

## Why We Neglect Context

If context is so critical, why do we undervalue it?

**Human-to-human problem:** We’ve all asked for help and been met with tedious follow-ups:  
> What version? What did you try? What’s your environment?  

Those clarifications are the tax we pay for weak context. We neglect context because:  
- We don’t fully grasp our own situation.  
- We assume others share our mental model.  
- We take shortcuts.  

**Human-to-AI amplification:** With AIs, the problem worsens. Many treat LLMs like search engines: fire off a vague prompt, expect a precise answer. But LLMs aren’t deterministic databases.  

As Thorsten Ball noted:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">What I find endlessly fascinating:<br /><br />Some engineers really can&#39;t seem to grasp that LLMs are non-deterministic and how to build software taking that into account.<br /><br />For others it immediately clicked, but for some it seems like there&#39;s a real mental barrier to accept it.</p>&mdash; Thorsten Ball (@thorstenball) <a href="https://twitter.com/thorstenball/status/1956017792484040731?ref_src=twsrc%5Etfw">August 14, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


This is the heart of why AI requires explicit context. Models have quirks we can’t ignore:

- **Nondeterminism:** The same input may yield different outputs—probabilistic, not fixed.  
- **Knowledge cutoffs:** They only know up to their training date, and need tools for real-time data.  
- **Hallucinations:** With weak context, they confidently invent plausible but false answers.  

Without grasping these limits, people assume “the model should just know.” In reality, rich context and the right tools are the only levers to counter nondeterminism, cutoffs, and hallucinations.  

---

## Context as Leverage

In the pre-AI era, the bottleneck was usually knowledge or tools. You had to learn more, buy better equipment, or gain access to new resources. AI flips the script. Models already come with vast general knowledge, and we can give them tools—but those tools only matter if paired with the right context. 

Tobi Lütke captured it well:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">I really like the term "context engineering" over prompt engineering. <br /><br />It describes the core skill better: the art of providing all the context for the task to be plausibly solvable by the LLM.</p>&mdash; tobi lutke (@tobi) <a href="https://twitter.com/tobi/status/1935533422589399127?ref_src=twsrc%5Etfw">June 19, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


“Context engineering” isn’t a catchphrase—it names the new constraint. Knowledge is abundant, and tools can be wired in, but without clear, explicit context, both remain dormant. Context is the multiplier that activates them and turns potential into outcomes.

---

## Conclusion

The **task-solving equation** highlights the shift of the AI era. Success no longer hinges on accumulating more knowledge or better tools—those are increasingly commoditized. What’s scarce is the ability to provide rich, explicit context.

Humans succeed despite limited knowledge because we’re rich in context and adaptable with tools. AIs succeed only when we deliberately supply the missing context. Every assumption, every nuance, every artifact matters.

As Guillermo Rauch observed:

<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">Imagine success being determined purely based on the quality of your thoughts. That's the promise of AI</p>&mdash; Guillermo Rauch (@rauchg) <a href="https://twitter.com/rauchg/status/1956356467898114443?ref_src=twsrc%5Etfw">August 15, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


The quality of your thoughts—expressed through clear context—now determines success.  

**Clear context is leverage. In a world where knowledge is cheap and tools are abundant, context is the multiplier that decides outcomes.**